{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dimensionality Reduction\n",
    "\n",
    "Dimensionality reduction is a technique used in machine learning and statistics to reduce the number of input variables in a dataset. More input features often make a model more complex, increasing the risk of overfitting and making the model harder to interpret. Dimensionality reduction techniques aim to simplify models without losing much information.\n",
    "\n",
    "---\n",
    "\n",
    "### Why is Dimensionality Reduction Important?\n",
    "\n",
    "1. **Simplification**: Makes the dataset easier to explore and visualize.\n",
    "2. **Speed**: Reduces the computational complexity, making algorithms run faster.\n",
    "3. **Data Compression**: Allows for more efficient storage of data.\n",
    "4. **Noise Reduction**: Helps to eliminate irrelevant features or reduce noise.\n",
    "5. **Improved Performance**: Can lead to better model performance when irrelevant features are removed.\n",
    "\n",
    "---\n",
    "\n",
    "### Example: Customer Segmentation in Retail\n",
    "\n",
    "Imagine you're a data scientist at a retail company. The company collects data on customer transactions, online activity, customer service interactions, and more. In total, you have hundreds of features for each customer.\n",
    "\n",
    "**The Problem**: You want to segment your customer base to target marketing more effectively, but the high dimensionality of your data makes it difficult to gain any meaningful insights.\n",
    "\n",
    "**The Solution**: By applying dimensionality reduction techniques like PCA, you can reduce your hundreds of features to just a few principal components. These components can effectively summarize the essential information in the dataset, making it easier to perform customer segmentation.\n",
    "\n",
    "**Outcome**: The marketing team can now more effectively tailor strategies for different segments, thereby increasing customer engagement and revenue.\n",
    "\n",
    "## Overview\n",
    "\n",
    "1. The curse of dimensionality\n",
    "2. PCA\n",
    "3. Factor Analysis\n",
    "4. Adaptive Methods: t-SNE and UMAP\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Curse of Dimensionality\n",
    "\n",
    " In high dimensions, the behavior of data can be counterintuitive, and algorithms that work well in low dimensions can become ineffective or computationally expensive.  This is because the available \"space\" that training samples are embedded grows exponentially with respect to the data.  This has several ramifications:\n",
    "\n",
    " - Distances between items become larger\n",
    " - Difference between distances become smaller\n",
    " - The amount of data required to cover the space grows exponentially\n",
    "\n",
    " See [this explanation](https://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/) to help guide your intuitions.\n",
    "\n",
    "### The Hughes Phenomenon ###\n",
    "\n",
    "In the late 70’s, G. Hughes observed a strange phonomenon while observing large datasets. It is best summarized by the picture below. \n",
    "\n",
    "![The Hughes Phenomenon](./assets/hughes_phenomenon.png)\n",
    "\n",
    "Intuitively, it would be easy to understand that the more inputs (features) you provide the model, the more the predictive power of the model. However, after a certain point the accuracy of the prediction drops off. This is the essence of Hughes Phenomonon. It is based on the fact that measuring data (features or variables) typically has some level of error to it. When you compound this error over a large number of variables, the error explodes so much that the accuracy is affected.\n",
    "\n",
    "### It Depends!\n",
    "\n",
    "The curse of dimensionality depends heavily on the distribution of data in a space, the measurements used to calculate distance, and the algorithm applied.  For instance, KNN is highly susceptible to the curse of dimensionality, and Euclidean distances tend to perform worse than either Manhattan or cosine.  So, it's always worth exploring your data a little before you decide whether or not to worry about it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Experiments with dimensionality\n",
    "\n",
    "The following code illustrates how increasing dimensionality increases distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_points(num_points, num_dimensions):\n",
    "    return[np.random.randint(-100, 100, num_dimensions) for _ in range(num_points)]\n",
    "\n",
    "def euclidean_distance(point1, point2):\n",
    "    return np.sqrt(np.sum((point1 - point2) ** 2))\n",
    "\n",
    "def compute_pairwise_distances(points_x,points_y):\n",
    "    distances = []\n",
    "    for i,j in zip(points_x,points_y):\n",
    "        distances.append(euclidean_distance(i,j))\n",
    "    return np.array(distances)\n",
    "\n",
    "# Set the number of points and dimensions\n",
    "num_points = 10000\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Loop through a few different dimensions\n",
    "for ax, num_dimensions in zip(axes, [2, 5, 50]):\n",
    "    # Generate points in unit hypercube\n",
    "    points_x = generate_points(num_points, num_dimensions)\n",
    "    points_y = generate_points(num_points, num_dimensions)\n",
    "\n",
    "    # Compute pairwise distances\n",
    "    distances = compute_pairwise_distances(points_x,points_y)\n",
    "    \n",
    "    # Plot histogram of distances\n",
    "    ax.hist(distances, bins=30, edgecolor='black')\n",
    "    ax.set_title(f\"{num_dimensions}-D Space\")\n",
    "    ax.set_xlabel(\"Distance\")\n",
    "    ax.set_ylabel(\"Frequency\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "1. Can you create a loop and evaluate the mean distance for dimensions from 0 - 500? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, observe that the relative differences between max an min in fact approach zero!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import math\n",
    "\n",
    "deltas = []\n",
    "for N in range(2,50):\n",
    "    # Generate 1000 random points in N dimensions.\n",
    "    P = [np.random.randint(-100, 100, N) for _ in range(10000)]\n",
    "    Q = np.random.randint(-100,100,N)\n",
    "    diffs = [np.linalg.norm(p-Q) for p in P]\n",
    "    mxd = max(diffs)\n",
    "    mnd = min(diffs)\n",
    "    delta = (mxd-mnd)/mnd\n",
    "    deltas.append( delta )\n",
    "\n",
    "plt.plot(range(2,50),deltas)\n",
    "plt.xlabel('Number of dimensions')\n",
    "plt.ylabel('Euclidean Distance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Principle Components Analysis (PCA)\n",
    "\n",
    "One way to reduce dimensionality is a principle components analysis (PCA).  PCA works by performing an eigenvector/eigenvalue decomposition of the data's covariance matrix. The goal is to transform the original features into a new set of uncorrelated features, known as principal components, which capture as much of the data's variance as possible. The first few principal components typically capture the majority of the variance, allowing for a lower-dimensional representation of the data.\n",
    "\n",
    "### How does it work?\n",
    "\n",
    "1. Center the data around the origin by subtracting the mean of each feature from the data points.\n",
    "\n",
    "2. Calculate the covariance matrix, which captures how each feature varies with every other feature.\n",
    "\n",
    "3. Compute the eigenvalues and eigenvectors of this covariance matrix. An eigenvector of a square matrix $A$ is a non-zero vector $\\mathbf{v}$ that, when the matrix multiplies it, only scales the vector and does not change its direction. Mathematically, this can be written as $A\\mathbf{v} = \\lambda \\mathbf{v}$\n",
    "\n",
    "   Here, $A$ is the square matrix, $\\mathbf{v}$ is the eigenvector, and $\\lambda$ is the eigenvalue corresponding to this eigenvector. The eigenvalue controls how much the eigenvector is scaled. \n",
    "   \n",
    "   **Intuition**: Imagine you have a rubber sheet that you can stretch, compress, or rotate. If you put an arrow (vector) on this sheet and then transform the sheet, most arrows would change both direction and length. However, some special arrows (eigenvectors) would only get \"stretched\" or \"compressed\" — they wouldn't change direction. The amount by which they get stretched or compressed is the eigenvalue. \n",
    "\n",
    "4. Eigenvalues are sorted in descending order, and the eigenvectors are rearranged correspondingly. The first eigenvalue will be the largest and indicates the maximum variance in the data that the first principal component (the corresponding eigenvector) captures.\n",
    "\n",
    "5. To reduce dimensions, you can now select the first $k$ eigenvectors, where $k$ is the number of dimensions you want in your reduced dataset. You then project your original, centered data into this new \\(k\\)-dimensional subspace.\n",
    "\n",
    "6. You can approximate the original data from the reduced data by projecting it back onto the original high-dimensional space by multiplying (dot-product) the transformed data with the transpose of the reduced eigenvector matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Simple Example\n",
    "\n",
    "First, we'll set up some sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Step 1: Create synthetic 2D data\n",
    "np.random.seed(0)\n",
    "X = np.dot(np.random.rand(2, 2), np.random.randn(2, 200)).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the PCA, and the reconstruction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 2: Apply PCA and plot the rotated data\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Step 3: Apply PCA and plot the rotated data\n",
    "X_reduced = PCA(n_components=1).fit_transform(X)\n",
    "X_reconstructed = PCA(n_components=1).fit(X).inverse_transform(X_reduced)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, plotting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.6)\n",
    "plt.title(\"Original Data\")\n",
    "plt.xlabel(\"X1\")\n",
    "plt.ylabel(\"X2\")\n",
    "plt.axis('equal')\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.6)\n",
    "plt.title(\"After PCA Rotation\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.axis('equal')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(X_reconstructed[:, 0], X_reconstructed[:, 1], alpha=0.6)\n",
    "plt.title(\"Reduced to One Dimension\")\n",
    "plt.xlabel(\"X1\")\n",
    "plt.ylabel(\"X2\")\n",
    "plt.axis('equal')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting the right number of components\n",
    "\n",
    "To select the right number of components, we can use the eigenvalues to calculate the amount of variance explained.  The variance explained by each principal component is directly related to the eigenvalues of the covariance matrix of the original dataset. Specifically, the proportion of variance explained by the $i^{th}$ principal component is given by:\n",
    "\n",
    "$$\n",
    "\\text{Variance Explained}_i = \\frac{\\lambda_i}{\\sum_{j=1}^{n} \\lambda_j}\n",
    "$$\n",
    "\n",
    "where $\\lambda_i$ is the eigenvalue corresponding to the $i^{th}$ principal component and $n$ is the number of components.\n",
    "\n",
    "The eigenvalues are stored in decreasing order, and they measure the amount of variance along each principal component. A larger eigenvalue indicates that more variance is explained by that principal component.  We can visualize variance explained using a scree plot, and use the \"elbow\" method to select the number of components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Step 1: Create synthetic 10D data\n",
    "np.random.seed(0)\n",
    "X_high_dim = np.dot(np.random.rand(10, 10), np.random.randn(10, 200)).T\n",
    "\n",
    "# Step 2: Apply PCA\n",
    "pca_high_dim = PCA()  # although you can specify n_components, leaving out this parameter returns all\n",
    "X_pca_high_dim = pca_high_dim.fit_transform(X_high_dim)\n",
    "\n",
    "# Step 3: Calculate Variance Explained\n",
    "variance_explained = pca_high_dim.explained_variance_ratio_\n",
    "print(\"Variance Explained per Principal Component:\", variance_explained)\n",
    "\n",
    "# Step 4: Scree Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(variance_explained) + 1), variance_explained)\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Proportion of Variance Explained')\n",
    "plt.title('Scree Plot')\n",
    "plt.xticks(range(1, len(variance_explained) + 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, if you know how much variance you want to explain, you can easily calculate it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cumulative variance explained\n",
    "cumulative_variance_explained = np.cumsum(pca_high_dim.explained_variance_ratio_)\n",
    "\n",
    "# Find the number of components for desired explained variance\n",
    "threshold = 0.95  # 95% variance\n",
    "n_components = np.argmax(cumulative_variance_explained >= threshold) + 1\n",
    "print(f\"Number of components needed for {threshold*100}% variance: {n_components}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit learn also let's you specify the variance you want in advance and will automatically lift out the number of components you need, as long as your svd_solver is \"full\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_high_dim = PCA(threshold)  # although you can specify n_components, leaving out this parameter returns all\n",
    "X_pca_high_dim_auto = pca_high_dim.fit_transform(X_high_dim)\n",
    "\n",
    "print(f\"Number of components retrieved for {threshold*100}% = {X_pca_high_dim_auto.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "The MNIST data contains a large sample of handwritten letters.  Each letter is a 28x28 (768) grid of pixels, so the data is quite high dimensional.  Try using PCA to visualize the data.  I'll get you started with a little code below/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Load MNIST data\n",
    "mnist = fetch_openml(\"mnist_784\", version=1)\n",
    "X, y = mnist[\"data\"].values, mnist[\"target\"].astype(int).values\n",
    "\n",
    "# Sample the dataset to speed up computation (Optional)\n",
    "X, y = X[:7000], y[:7000]\n",
    "\n",
    "# Split the dataset\n",
    "X_mnist_train, X_mnist_test, y_mnist_train, y_mnist_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 2: Visualize using PCA (2 components)\n",
    "# Reduce the data with PCA\n",
    "\n",
    "# You need to populate the following variable\n",
    "X__mnist_reduced = None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now plotting\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.scatter(X_mnist_reduced[:, 0], X_mnist_reduced[:, 1], c=y_mnist_train, cmap=\"jet\",s=5)\n",
    "plt.colorbar()\n",
    "plt.title(\"MNIST - 2D PCA\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the scree plot..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 3: Scree Plot\n",
    "pca_scree = None # What goes here???\n",
    "# How do you get variance explained?\n",
    "plt.xlabel(\"Number of Components\")\n",
    "plt.ylabel(\"Cumulative Variance Explained\")\n",
    "plt.title(\"Scree Plot\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the number of components to achieve 95% variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 4: Number of components for 95% variance\n",
    "threshold = 0.95\n",
    "n_components = None # What goes here???\n",
    "print(f\"Number of components for {threshold*100}% variance: {n_components}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's interesting to look at how a transform and reconstruction affects a single MNIST digit.  I've written a function to display a single digit.  Use PCA and the reconstruction method to have a look at this.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mnist_digit(digit):\n",
    "    \"\"\"Plot a single MNIST digit.\n",
    "\n",
    "    Parameters:\n",
    "    digit (numpy array): A flattened 1D numpy array of length 784.\n",
    "\n",
    "    \"\"\"\n",
    "    # Reshape the flattened digit to 28x28 image\n",
    "    digit_image = digit.reshape(28, 28)\n",
    "    \n",
    "    plt.imshow(digit_image, cmap='binary')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "plot_mnist_digit(X_mnist_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use PCA with n_components to transform the above digit.  How does it look?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `n_components` to train a KNNClassifier, and compare accuracy to the non-reduced data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: k-NN Classification\n",
    "# Without PCA\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_mnist_train, y_mnist_train)\n",
    "y_pred = knn.predict(X_mnist_test)\n",
    "print(f\"Original Data Accuracy: {accuracy_score(y_mnist_test, y_pred)}\")\n",
    "\n",
    "# With PCA\n",
    "# You have some work to do right here\n",
    "\n",
    "\n",
    "y_pred_pca = None # How do you do this?\n",
    "print(f\"PCA-transformed Data Accuracy: {accuracy_score(y_mnist_test, y_pred_pca)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Limitations of PCA\n",
    "\n",
    "One of the key limitations of PCA is that it is a linear technique. This means that it works by finding new axes, called principal components, that are linear combinations of the original features. These principal components aim to capture as much variance in the data as possible. However, the linear nature of PCA imposes a limitation: it struggles to capture patterns in data that are fundamentally non-linear.\n",
    "\n",
    "Imagine a dataset where points are distributed in a non-linear fashion, like a spiral or concentric circles. A linear method like PCA would not be able to capture the essence of such patterns. This is because the concept of \"distance\" or \"variance\" that PCA relies on does not adequately describe the intrinsic geometry of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Generate synthetic spiral data\n",
    "n_points = 1000\n",
    "n_turns = 3\n",
    "t = np.linspace(0, n_turns * np.pi, n_points)\n",
    "x = t * np.cos(t)\n",
    "y = t * np.sin(t)\n",
    "\n",
    "X_spiral = np.column_stack([x, y])\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_spiral)\n",
    "\n",
    "# Scree Plot\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(range(1, len(pca.explained_variance_ratio_)+1), pca.explained_variance_ratio_)\n",
    "plt.title(\"Scree Plot\")\n",
    "plt.xlabel(\"Component Number\")\n",
    "plt.ylabel(\"Variance Explained\")\n",
    "\n",
    "# Original Data vs PCA reconstruction\n",
    "X_reduced = PCA(n_components=1).fit_transform(X_spiral)\n",
    "X_reconstructed = PCA(n_components=1).fit(X_spiral).inverse_transform(X_reduced)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_spiral[:, 0], X_spiral[:, 1], alpha=0.6, label='Original Data')\n",
    "plt.scatter(X_reconstructed[:, 0], X_reconstructed[:, 1], alpha=0.6, label='PCA Reconstructed')\n",
    "plt.legend()\n",
    "plt.axis('equal')\n",
    "plt.title(\"Original vs PCA Reconstructed Data\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Other Approaches\n",
    "\n",
    "There are many many approach to dimesionality reduction.  Several extensions to PCA address it's limitations and extend it's capabilities.  One of these is **Kernel PCA**.\n",
    "\n",
    "### Kernel PCA\n",
    "\n",
    "Kernel Principal Component Analysis (Kernel PCA) is an extension of the standard Principal Component Analysis (PCA), designed to handle non-linear data effectively. While standard PCA works by finding linear principal components to project data onto, Kernel PCA works in an implicitly defined higher-dimensional feature space where these projections can be non-linear.\n",
    "\n",
    "### How Kernel PCA Works\n",
    "\n",
    "1. **Feature Space Mapping**: First, each data point in the original space is mapped into a higher-dimensional feature space using a kernel function $K(x_i, x_j)$. These are the same as with **Support Vector Machines**.\n",
    "\n",
    "2. **Principal Component Analysis in Feature Space**: In this new feature space, the regular PCA algorithm is applied. Since working in a higher-dimensional space can be computationally intensive, Kernel PCA cleverly utilizes the \"kernel trick\" to compute principal components in the feature space without explicitly working in it.\n",
    "\n",
    "3. **Projecting Data**: Finally, the original data is projected onto these new principal components in the feature space. The resulting data can be linearly separable even if the original data was not.\n",
    "\n",
    "### Kernel Functions\n",
    "\n",
    "Kernel PCA relies on the use of a kernel function to compute the similarity between pairs of data points in the feature space. Popular choices for kernel functions include:\n",
    "\n",
    "- **Polynomial Kernel**: $K(x_i, x_j) = (x_i \\cdot x_j + c)^d$\n",
    "- **RBF (Radial Basis Function) or Gaussian Kernel**: $K(x_i, x_j) = \\exp(-\\gamma ||x_i - x_j||^2)$\n",
    "- **Sigmoid Kernel**: $K(x_i, x_j) = \\tanh(\\alpha x_i \\cdot x_j + c)$\n",
    "\n",
    "### Advantages and Limitations\n",
    "\n",
    "#### Advantages\n",
    "- Can capture complex, non-linear relationships in data.\n",
    "- Suitable for clustering, classification, and other tasks where linear methods fail.\n",
    "\n",
    "#### Limitations\n",
    "- Computationally more expensive compared to standard PCA.\n",
    "- Requires choosing an appropriate kernel function and tuning its parameters, which may not be straightforward.\n",
    "\n",
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.decomposition import KernelPCA, PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Generate synthetic data for concentric circles\n",
    "np.random.seed(0)\n",
    "X, y = make_circles(n_samples=400, factor=.3, noise=.05)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_std = scaler.fit_transform(X)\n",
    "\n",
    "# Apply regular PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_std)\n",
    "\n",
    "# Apply Kernel PCA with Radial Basis Function (RBF) kernel\n",
    "kpca = KernelPCA(kernel=\"rbf\", gamma=1, n_components=2)\n",
    "X_kpca = kpca.fit_transform(X_std)\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot original data\n",
    "axes[0].scatter(X_std[y == 0, 0], X_std[y == 0, 1], color='red', marker='^', alpha=0.5)\n",
    "axes[0].scatter(X_std[y == 1, 0], X_std[y == 1, 1], color='blue', marker='o', alpha=0.5)\n",
    "axes[0].set_title(\"Original Data\")\n",
    "\n",
    "# Plot data transformed by PCA\n",
    "axes[1].scatter(X_pca[y == 0, 0], X_pca[y == 0, 1], color='red', marker='^', alpha=0.5)\n",
    "axes[1].scatter(X_pca[y == 1, 0], X_pca[y == 1, 1], color='blue', marker='o', alpha=0.5)\n",
    "axes[1].set_title(\"After PCA\")\n",
    "\n",
    "# Plot data transformed by Kernel PCA\n",
    "axes[2].scatter(X_kpca[y == 0, 0], X_kpca[y == 0, 1], color='red', marker='^', alpha=0.5)\n",
    "axes[2].scatter(X_kpca[y == 1, 0], X_kpca[y == 1, 1], color='blue', marker='o', alpha=0.5)\n",
    "axes[2].set_title(\"After Kernel PCA\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive Techniques\n",
    "\n",
    "Adaptive techniques seek to preserve complex, non-linear relationships between data points. Unlike linear methods like PCA, which apply a rigid transformation to all points in the dataset equally, adaptive methods tweak and optimize the transformation for different regions of the data space. In simpler terms, they adapt to the underlying data distribution.\n",
    "\n",
    "This adaptability often makes these techniques more suitable for capturing and preserving intricate structures in high-dimensional data, such as clusters or manifolds.  However, they are often stochastic and are not guaranteed to preserve distances in the transformed data, so may not be suitable in all cases.  They are, however, excellent for visualization and some kinds of classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## t-SNE (t-Distributed Stochastic Neighbor Embedding)\n",
    "\n",
    "t-SNE is particularly effective for visualizing high-dimensional data in 2D or 3D.\n",
    " \n",
    "#### How it Works:\n",
    "\n",
    "1. **Probability Distributions**: t-SNE starts by calculating the pairwise similarities between points in the high-dimensional space and in the low-dimensional space. These similarities are converted into conditional probabilities that a point would pick another point as its neighbor if neighbors were picked in proportion to their similarity.\n",
    "\n",
    "2. **Minimize the Divergence**: t-SNE aims to minimize the difference between these two probability distributions for the high-dimensional and low-dimensional spaces. Specifically, it minimizes the Kullback-Leibler divergence between them.\n",
    "\n",
    "3. **t-Distribution**: To calculate the similarity between points in the low-dimensional space, t-SNE uses a t-distribution, which has heavier tails compared to a normal distribution. This allows t-SNE to be particularly sensitive to local structures.\n",
    "\n",
    "#### Strengths:\n",
    "\n",
    "- Excellent at preserving local cluster structures.\n",
    "- Particularly useful for visualization.\n",
    "\n",
    "#### Weaknesses:\n",
    "\n",
    "- Computationally intensive.\n",
    "- May not preserve global structures well.\n",
    "- The results are sensitive to hyperparameters like perplexity and the learning rate.\n",
    "- **IMPORTANT** t-SNE does not enable \"training\" - you cannot fit a model a and then apply it to new data.  As a result, it is not really appopriate for train / test scenarios.\n",
    "\n",
    "t-SNE has been a popular choice for tasks like visualizing gene expression data, understanding neural network activations, clustering, and much more.\n",
    "\n",
    "With t-SNE, it's easier to capture the non-linear manifolds and groupings in data, making it an excellent tool for exploratory data analysis. However, because it's computationally demanding and somewhat unpredictable, it may not always be the best choice for all types of dimensionality reduction tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Example\n",
    "\n",
    "A classic example of data that PCA struggles with is the `swiss roll`, which you can envision as a sheet of paper rolled into a tube.  You'll see the PCA manages to uncover the spiral, but not it's \"sheet-like\" properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.datasets import make_swiss_roll\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Create Swiss Roll data\n",
    "n_samples = 1500\n",
    "noise = 0.05\n",
    "X, color = make_swiss_roll(n_samples, noise=noise)\n",
    "# Make it thinner\n",
    "X[:, 1] *= .5\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Apply t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "\n",
    "# Visualization\n",
    "fig = plt.figure(figsize=(18, 6))\n",
    "\n",
    "# 3D Swiss Roll\n",
    "ax0 = fig.add_subplot(131, projection='3d')\n",
    "ax0.scatter(X[:, 0], X[:, 1], X[:, 2], c=color, cmap=plt.cm.Spectral)\n",
    "ax0.set_title(\"Original 3D Swiss Roll\")\n",
    "\n",
    "# PCA\n",
    "ax1 = fig.add_subplot(132)\n",
    "ax1.scatter(X_pca[:, 0], X_pca[:, 1], c=color, cmap=plt.cm.Spectral)\n",
    "ax1.set_title(\"PCA of Swiss Roll\")\n",
    "\n",
    "# t-SNE\n",
    "ax2 = fig.add_subplot(133)\n",
    "ax2.scatter(X_tsne[:, 0], X_tsne[:, 1], c=color, cmap=plt.cm.Spectral)\n",
    "ax2.set_title(\"t-SNE of Swiss Roll\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "Try using t-SNE to visualize the MNIST data above.  How does it compare to PCA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, try runing the KNN algorithm again.  How does the classifier perform now?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UMAP (Uniform Manifold Approximation and Projection)\n",
    "\n",
    "UMAP is a newer technique for dimensionality reduction and data visualization, like t-SNE. It is designed to provide more robust results and be computationally efficient, making it suitable for larger datasets.\n",
    "\n",
    "### Key Features of UMAP:\n",
    "\n",
    "1. **Preservation of Topology:** UMAP aims to preserve not just local but also global data structures. The emphasis on global / local structure can be controlled by parameters. In comparison, t-SNE excels at preserving local structures but may not always capture the global relationships as accurately.\n",
    "\n",
    "2. **Computational Efficiency:** UMAP is generally faster than t-SNE, especially for larger datasets, making it a more scalable option.\n",
    "\n",
    "3. **Versatility:** UMAP is not just for visualization; you can also use it for general non-linear dimensionality reduction tasks.\n",
    "\n",
    "4. **Compatibility with Metrics:** Unlike t-SNE, which mainly uses Euclidean distance, UMAP can work with a variety of distance metrics.\n",
    "\n",
    "### Working Principle:\n",
    "\n",
    "1. **Mathematical Foundations:** UMAP is grounded in rigorous mathematics, specifically the theory of Riemannian geometry and algebraic topology.\n",
    "\n",
    "2. **Manifold Learning:** Similar to t-SNE, UMAP also operates on the principle that the data is sampled from some low-dimensional manifold embedded in a high-dimensional space. It works to learn this manifold structure.\n",
    "\n",
    "3. **Graph-based Approach:** UMAP first constructs a weighted k-NN graph for the data in the original space. It then optimizes a similar graph in the lower-dimensional space to be as structurally similar to the original graph as possible.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>Low dimensional manifolds</b>: A \"low-dimensional manifold\" refers to a manifold of lower dimensionality than the space in which it is embedded. For example, consider a two-dimensional sheet of paper that is crumpled and placed in a three-dimensional room. Despite the crumpling, each point on the paper still has a neighborhood that resembles a flat, 2D plane. Thus, the crumpled paper represents a 2D manifold within the 3D space of the room.\n",
    "\n",
    "The concept of a low-dimensional manifold is often invoked to suggest that the meaningful dimensions of the data are fewer than the number of columns in the data table. For instance, you might have a 1,000-dimensional dataset that actually lies along a curved, 10-dimensional surface within that 1,000-dimensional space. Machine learning techniques like manifold learning aim to discover this low-dimensional surface (or manifold), making the data easier to visualize, understand, and work with.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You'll need to install UMAP learn for UMAP\n",
    "# Unfortunately, UMAP learn currently depends on an older version of numpy\n",
    "# To handle this, you'll want to create a new environment, install the necessary libraries\n",
    "# and switch to this kernel for all processing\n",
    "\n",
    "!pip install numpy==1.24\n",
    "!pip install umap-learn\n",
    "!pip install matplotlib\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Here's an example with the `Swiss roll` - note that both seem to do a pretty good job, but UMAP is quite a bit faster!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_swiss_roll\n",
    "from sklearn.manifold import TSNE\n",
    "import umap\n",
    "from mpl_toolkits.mplot3d import Axes3D  # Import for 3D plotting\n",
    "import time\n",
    "\n",
    "# Generate Swiss roll data\n",
    "n_samples = 1500\n",
    "noise = 0.05\n",
    "X, color = make_swiss_roll(n_samples, noise=noise)\n",
    "\n",
    "# Apply t-SNE\n",
    "start_time_tsne = time.time()\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "end_time_tsne = time.time()\n",
    "\n",
    "# Apply UMAP\n",
    "start_time_umap = time.time()\n",
    "umap_model = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)\n",
    "X_umap = umap_model.fit_transform(X)\n",
    "end_time_umap = time.time()\n",
    "\n",
    "elapsed_tsne = end_time_tsne - start_time_tsne\n",
    "elapsed_umap = end_time_umap - start_time_umap\n",
    "print(f\"Time elapsed for t-SNE: {elapsed_tsne:.4f} seconds\")\n",
    "print(f\"Time elapsed for UMAP: {elapsed_umap:.4f} seconds\")\n",
    "\n",
    "\n",
    "# Plot the data\n",
    "fig = plt.figure(figsize=(18, 6))\n",
    "\n",
    "# Original Swiss roll in 3D\n",
    "ax1 = fig.add_subplot(131, projection='3d')\n",
    "ax1.scatter(X[:, 0], X[:, 1], X[:, 2], c=color, cmap=plt.cm.Spectral)\n",
    "ax1.set_title('Original Swiss Roll in 3D')\n",
    "ax1.set_xlabel('X-axis')\n",
    "ax1.set_ylabel('Y-axis')\n",
    "ax1.set_zlabel('Z-axis')\n",
    "\n",
    "# t-SNE plot\n",
    "ax2 = fig.add_subplot(132)\n",
    "ax2.scatter(X_tsne[:, 0], X_tsne[:, 1], c=color, cmap=plt.cm.Spectral)\n",
    "ax2.set_title('t-SNE')\n",
    "ax2.set_xlabel('t-SNE 1')\n",
    "ax2.set_ylabel('t-SNE 2')\n",
    "\n",
    "# UMAP plot\n",
    "ax3 = fig.add_subplot(133)\n",
    "ax3.scatter(X_umap[:, 0], X_umap[:, 1], c=color, cmap=plt.cm.Spectral)\n",
    "ax3.set_title('UMAP')\n",
    "ax3.set_xlabel('UMAP 1')\n",
    "ax3.set_ylabel('UMAP 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding umap parameters\n",
    "\n",
    "There is a wonderful [online tutorial](https://pair-code.github.io/understanding-umap/) that illustrates the impact of UMAP parameters.  I've replicated part of it here for you.  It uses data from a 3D scan of a Wooley Mammoth, curated by [Maximilian Noichl](https://homepage.univie.ac.at/maximilian.noichl/post/mammoth/).\n",
    "\n",
    "The basic idea here is that the `nearest neighbors` parameter adjust sensitivity to global structure, and `min-dist` parameter controls the \"fuzziness\" of the embedding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, here's the 3D mammoth\n",
    "import pandas as pd\n",
    "mammoth = pd.read_csv('./data/mammoth_a.csv')\n",
    "mammoth = mammoth.sample(50000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(90,72))\n",
    "\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "# ax.set_aspect('equal')\n",
    "\n",
    "ax.scatter(mammoth['x'], mammoth['y'], mammoth['z'], s=20,c='black')\n",
    "ax.view_init(20, -170)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embeddings take a while to process, especially when the number of neighbors increases, but here's an example with a relatively low number of nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "\n",
    "umap = umap.UMAP(    init='random',\n",
    "                    n_components=2,\n",
    "                    n_neighbors=30,\n",
    "                    min_dist=0.1,\n",
    "                     spread=2,\n",
    "                    metric='euclidean',\n",
    "                    verbose=True)\n",
    "\n",
    "mammoth_transformed = umap.fit(mammoth.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that UMAP has two methods, `fit` and `fit_transform`, like most scikit learn ML models.  With the `fit` method, UMAP will create a model that can then be applied to new data.  This is a kind of supervised learning, allowing you to build an embedding model on a small bit of data first, and then extending it larger data.  See the [UMAP documentation](https://umap-learn.readthedocs.io/en/latest/basic_usage.html#digits-data) page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20),facecolor='w')\n",
    "plt.axis('off')\n",
    "plt.scatter(mammoth_transformed.embedding_[:, 0], mammoth_transformed.embedding_[:, 1], s=1,color='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Maximilian Noichl](https://homepage.univie.ac.at/maximilian.noichl/post/mammoth/) generated a few animations that help illustrate how UMAP behaves across different parameter settings.\n",
    "\n",
    "#### Exploring Min Dist\n",
    "\n",
    "![Min dist animation](./assets/mammoth_md.gif)\n",
    "\n",
    "#### Exploring Nearest Neighbors\n",
    "\n",
    "![Min dist animation](./assets/mammoth_nn.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4\n",
    "\n",
    "Try using UMAP to process the MNIST data.  Visualize the data first, and then calculate accuracy with KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here - just use default parameters for UMAP\n",
    "import umap\n",
    "# Capture your data data in the following variable\n",
    "X_umap = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "plt.scatter(X_umap[:, 0], X_umap[:, 1], c=y_mnist_train, cmap=\"Spectral\",s=5)\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "plt.colorbar(boundaries=np.arange(11)-0.5).set_ticks(np.arange(10))\n",
    "plt.title(\"MNIST - 2D UMAP\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5\n",
    "\n",
    "Try exploring the impact of dimensionality reduction using the following sample data.\n",
    "\n",
    "1. How does a KNN classifier do as the number of dimensions increases?\n",
    "2. Apply a PCA that captures 90% of the variance. Does it help?\n",
    "3. Does UMAP help?\n",
    "4. What explains the difference? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(0)\n",
    "n_points = 500\n",
    "\n",
    "# Class 0\n",
    "x0 = np.random.normal(loc=0, scale=10, size=n_points)\n",
    "y0 = np.random.normal(loc=0, scale=10, size=n_points)\n",
    "z0 = np.random.normal(loc=0, scale=1, size=n_points)\n",
    "\n",
    "# Class 1\n",
    "x1 = np.random.normal(loc=0, scale=10, size=n_points)\n",
    "y1 = np.random.normal(loc=0, scale=10, size=n_points)\n",
    "z1 = np.random.normal(loc=2, scale=1, size=n_points)\n",
    "\n",
    "X = np.vstack((np.hstack((x0, x1)), np.hstack((y0, y1)), np.hstack((z0, z1)))).T\n",
    "y = np.hstack((np.zeros(n_points), np.ones(n_points)))\n",
    "\n",
    "\n",
    "\n",
    "# Train KNN on original data\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X, y)\n",
    "y_pred = knn.predict(X)\n",
    "print(f\"Original Data Accuracy: {accuracy_score(y, y_pred)}\")\n",
    "\n",
    "# Create a new figure\n",
    "fig = plt.figure(figsize=(15,15))\n",
    "\n",
    "# Add 3D subplot\n",
    "# The format is (rows, columns, plot_number)\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.view_init(elev=0, azim=30)\n",
    "\n",
    "ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=y, cmap='viridis')\n",
    "ax.set_title('Original Data')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
